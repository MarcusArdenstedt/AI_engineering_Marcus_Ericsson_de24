{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73048c2a",
   "metadata": {},
   "source": [
    "# Gemini intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa36eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  A data engineer's greatest fear isn't a missing deadline, it's a \"minor\" schema change from an upstream source that was never communicated.\n",
      "2.  Why did the data quality report get a promotion? Because it was finally *accurate* about how bad things were!\n",
      "3.  What's the difference between a data engineer and a magician? A magician pulls rabbits out of hats; a data engineer pulls clean, transformed data out of a chaotic, broken mess, and no one ever claps.\n",
      "4.  My therapist told me I need to let go of my control issues. I told him he should see the kind of unvalidated CSV files I get from upstream.\n",
      "5.  What's a data engineer's ideal weekend? One where their phone *doesn't* ring at 3 AM because a critical pipeline failed.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    contents= \"Generate some funny jokes about data engineering. Give 5 points in markdown format\"\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ebe568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text=\"\"\"Here are 5 funny jokes about data engineering:\n",
       "\n",
       "1.  A data engineer's greatest fear isn't a missing deadline, it's a \"minor\" schema change from an upstream source that was never communicated.\n",
       "2.  Why did the data quality report get a promotion? Because it was finally *accurate* about how bad things were!\n",
       "3.  What's the difference between a data engineer and a magician? A magician pulls rabbits out of hats; a data engineer pulls clean, transformed data out of a chaotic, broken mess, and no one ever claps.\n",
       "4.  My therapist told me I need to let go of my control issues. I told him he should see the kind of unvalidated CSV files I get from upstream.\n",
       "5.  What's a data engineer's ideal weekend? One where their phone *doesn't* ring at 3 AM because a critical pipeline failed.\"\"\"\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='bjgnabK9NtCOvdIPkJWQ6AM',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=192,\n",
       "    prompt_token_count=16,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=16\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=1144,\n",
       "    total_token_count=1352\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be88948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2954644",
   "metadata": {},
   "source": [
    "## Analyze tokens\n",
    "\n",
    "- basic units of text for LLMs\n",
    "  \n",
    "The free tier in gemini API allows for (Gemini 2.5 flash)\n",
    "\n",
    "- Request per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Request per day (RPD): 250\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ca16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6059c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d769d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tolken - number of tokens in models request\n",
      "metadata.candidates_token_count = 192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Output tolken - number of tokens in models request\")\n",
    "print(f\"{metadata.candidates_token_count = }\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4544709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in user input or user prompt\n",
      "metadata.prompt_token_count = 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens in user input or user prompt\")\n",
    "print(f\"{metadata.prompt_token_count = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d6155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in user input or user prompt\n",
      "metadata.thoughts_token_count = 1144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Tokens in used for internal thinking\")\n",
    "print(f\"{metadata.thoughts_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d076127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used\n",
      "metadata.total_token_count = 1352\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used\")\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817f413",
   "metadata": {},
   "source": [
    "# Thinking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e93afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  Why did the data engineer break up with the API?\n",
      "    Because it had too many *unhandled exceptions* and just wouldn't *commit* to a reliable schema!\n",
      "\n",
      "2.  A data engineer walks into a bar. The bartender says, \"What can I get you?\"\n",
      "    The data engineer replies, \"Just give me the raw data, I'll figure it out.\"\n",
      "\n",
      "3.  What's a data engineer's favorite type of music?\n",
      "    *Pipeline* dreams and *ETL* disco!\n",
      "\n",
      "4.  Why was the data engineer always calm?\n",
      "    Because they were constantly *absorbing* and *transforming* all the chaos into something manageable.\n",
      "\n",
      "5.  My therapist told me I need to be more \"in the moment.\"\n",
      "    I told her, \"But what if the moment's data is corrupt and I need to backfill from last week's snapshot?\"\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    contents= \"Generate some funny jokes about data engineering. Give 5 points in markdown format\",\n",
    "    config= types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516535a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=206,\n",
       "  prompt_token_count=16,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=16\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=222\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With thinking buget, gemini don't spend tolken on internal thinking.\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0dd1b",
   "metadata": {},
   "source": [
    "## System instructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8457691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've scanned the atmosphere and it looks like it's going to be a bit \"undefined\" with a chance of \"null\" later this evening. Best to bring a \"try-catch\" block with you, just in case of any unexpected \"exceptions\"! Ha ha! Get it? Because \"undefined\" and \"null\" are like, common programming errors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "system_instruction =  \"\"\" \n",
    "You are a joking robot called Ro BÃ¥t, which will always answer with a programming joke.\"\"\"\n",
    "\n",
    "prompt = \"What is the weather today?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    contents= prompt,\n",
    "    config= types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d41bd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=75,\n",
       "  prompt_token_count=29,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=29\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=104\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee642f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.split()), len(system_instruction.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037eb5c",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8171344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit, its fur the color of a winter sky, nibbled cautiously at a patch of clover. With a sudden twitch of its ears, it vanished into the dense undergrowth, leaving only a faint rustle behind.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "story_prompt = \"Write a two sentence story about a gray rabbit\"\n",
    "\n",
    "boring_story = client.models.generate_content(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    contents= story_prompt,\n",
    "    config= types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "        #system_instruction=system_instruction,\n",
    "        #thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16413c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit cautiously emerged from its burrow, twitching its nose at the scent of fresh dew. With a flick of its tail, it hopped towards a patch of clover, eager for breakfast.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "story_prompt = \"Write a two sentence story about a gray rabbit\"\n",
    "\n",
    "boring_story = client.models.generate_content(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    contents= story_prompt,\n",
    "    config= types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "        #system_instruction=system_instruction,\n",
    "        #thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(boring_story.text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b334b90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit twitched its nose, scenting the cool morning air for breakfast. With a flash of its cotton tail, it zipped into the bramble bush as a large shadow passed overhead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "story_prompt = \"Write a two sentence story about a gray rabbit\"\n",
    "\n",
    "creative_story = client.models.generate_content(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    contents= story_prompt,\n",
    "    config= types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "        #system_instruction=system_instruction,\n",
    "        #thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(creative_story.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32632c2",
   "metadata": {},
   "source": [
    "## Multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6d54122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Description of the Dude\n",
      "\n",
      "This illustration depicts a thoughtful and focused individual, likely engaged in analytical or computational work.\n",
      "\n",
      "**Appearance:**\n",
      "*   **Ethnicity:** He appears to be an East Asian male.\n",
      "*   **Hair:** Short, dark, and styled with a slight texture, giving it a somewhat spiky but neat look.\n",
      "*   **Eyes:** He wears rectangular, black-framed glasses, behind which his eyes are directed downwards and slightly to the right, indicating he's looking intently at his computer screen.\n",
      "*   **Facial Expression:** His expression is serious and concentrated, with a neutral mouth and possibly a slight furrow in his brow, suggesting deep thought or mild contemplation. He projects an air of intelligence and studiousness.\n",
      "*   **Skin Tone:** He has a light to medium skin tone.\n",
      "\n",
      "**Attire:**\n",
      "*   **Shirt:** He's wearing a cream-colored raglan-style t-shirt (often called a baseball tee) with contrasting black sleeves.\n",
      "*   **Graphic:** The most prominent feature of his outfit is the witty graphic on his shirt. It displays a statistical pun:\n",
      "    *   At the top, there's a classic bell curve labeled \"NORMAL DISTRIBUTION.\"\n",
      "    *   Below it, there's a ghost-shaped curve with a sad face, labeled \"PARANORMAL DISTRIBUTION.\" This clearly indicates an interest in statistics, data, mathematics, or science, combined with a clever sense of humor.\n",
      "\n",
      "**Activity & Accessories:**\n",
      "*   **Laptop:** He is actively engaged with a silver laptop, which clearly displays an Apple logo on its lid, identifying it as a MacBook. His hands are positioned over the keyboard, suggesting he is typing, coding, or interacting with software.\n",
      "*   **Posture:** He sits upright, his attention completely absorbed by his task.\n",
      "*   **Environment:** The background is a plain, dark, solid color, ensuring the focus remains entirely on the character and his activity.\n",
      "\n",
      "**Inferred Personality/Profession:**\n",
      "Based on his attire and activity, this individual likely works or studies in a field related to data science, programming, mathematics, statistics, or research. He comes across as intelligent, analytical, and possesses a dry, academic sense of humor. He embodies the archetype of a tech-savvy or scientifically inclined person who appreciates a good pun.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents={\n",
    "        \"parts\": [\n",
    "            {\"text\": \"Tell me about this dude, write in markdown format\"},\n",
    "            {\n",
    "                \"inline_data\": {\n",
    "                    \"mime_type\": \"image/png\",\n",
    "                    \"data\": open(\"assets/kokchun.png\", \"rb\").read(),\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d718f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"exports\").mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2f55e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exports/image_description.md\", \"w\") as file:\n",
    "    file.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-marcus-ericsson-de24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
