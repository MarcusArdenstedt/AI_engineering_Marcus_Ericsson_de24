{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c90007da",
   "metadata": {},
   "source": [
    "# Gemini API intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da66450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# looks automatically after the key\n",
    "# GOOGLE_API_KEY and GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def ask_gemini(prompt, model= \"gemini-2.5-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "response = ask_gemini(\"Give me 5 some data engineering jokes, structrure it in short points\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# knows that GenerateContentResponse is a pydantic model\n",
    "# -> can work with it in a OOP manner\n",
    "isinstance(response,BaseModel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sama as dict(response).keys()\n",
    "\n",
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da044a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "response.sdk_http_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855441c8",
   "metadata": {},
   "source": [
    "# Tokens\n",
    "\n",
    "- basic unit of text for LLMs\n",
    "- can be as short as one character or as long as one word\n",
    "\n",
    "- tokens used for billing \n",
    "\n",
    "Gemini free tier\n",
    "- Requests per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Request per day (RPD): 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thinking is expensive\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf0464",
   "metadata": {},
   "source": [
    "## Thinking \n",
    "\n",
    "- hyperparameter to allocate more compute for complex task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "\n",
    "prompt =\"Give me 5 some data engineering jokes, structrure it in short points\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config= types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01563a",
   "metadata": {},
   "source": [
    "## System intstruction\n",
    "- hyperparameter to guide model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b98083",
   "metadata": {},
   "outputs": [],
   "source": [
    "System_instruction = \"\"\"\n",
    "You are expert in python programming, you will always provide idiomatic code i.e.\n",
    "pythonic code. So when you see my code or my question, be very critical, but answer \n",
    "in a concise way. Also be constructive to help me improve\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Explain OOP and dunder methods\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt,\n",
    "    config= types.GenerateContentConfig(\n",
    "        system_instruction=System_instruction    \n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481eac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{meta_data.candidates_token_count = }\") # outnput\n",
    "print(f\"{meta_data.prompt_token_count =}\")    # prompt + system intruction \n",
    "print(f\"{meta_data.total_token_count =}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt.split()), len(System_instruction.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367c247",
   "metadata": {},
   "source": [
    "# Temperature\n",
    "\n",
    "- controls randomness of output -> 'creative'\n",
    "\n",
    "- it's a hyperparameter that can be adjust to infuence the diversity and creativity of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942bc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Write 2 story about a gray rabbit\"\n",
    "\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config= types.GenerateContentConfig(\n",
    "        temperature=2   \n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401664a",
   "metadata": {},
   "source": [
    "# Multimodel input\n",
    "\n",
    "input text and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57986f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"Describe this image shortly\"\n",
    "image_input = {\"mime_type\": \"image/png\", \"data\": open(\"bella.png\", 'rb').read()}\n",
    "\n",
    "respone = client.models.generate_content(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    contents= dict(\n",
    "        parts=[dict(text = text_input), dict(inline_data = image_input)]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(respone.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-marcus-ericsson-de24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
